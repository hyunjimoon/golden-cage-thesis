# ğŸ“ˆ Time Series Implementation & Visualization Guide
> **í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: D_t, A_t, G_tëŠ” ì‹œê³„ì—´ ë³€ìˆ˜ì…ë‹ˆë‹¤. ì´ temporal structureë¥¼ í™œìš©í•˜ë©´ ì¸ê³¼ ê´€ê³„ë¥¼ ë” ê°•í•˜ê²Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> **Author**: ğŸ… ê¶Œì¤€ (Claude Code)

---

## ğŸ“Š ë°ì´í„°ì˜ ì‹œê³„ì—´ êµ¬ì¡°

### í˜„ì¬ Panel Structure
```
Years:  t=0 (2021) â†’ t=1 (2023) â†’ t=2 (2024) â†’ t=3 (2025)
        â†“           â†“           â†“           â†“
        Vâ‚€          Vâ‚          Vâ‚‚          Vâ‚ƒ
        Eâ‚€          -           -           -
        -           Î”Vâ‚         Î”Vâ‚‚         Î”Vâ‚ƒ
        -           Dâ‚          Dâ‚‚          Dâ‚ƒ (=total_delta_V)
```

### ë³€ìˆ˜ ì •ì˜ (ì‹œê³„ì—´ ê´€ì )
| ë³€ìˆ˜ | ì •ì˜ | ì‹œê³„ì—´ íŠ¹ì„± |
|------|------|------------|
| **V_t** | ì‹œì  tì˜ Vagueness | Stock variable (ìƒíƒœ) |
| **Î”V_t** | V_t - V_{t-1} | Flow variable (ë³€í™”) |
| **D_t** | V_t - V_0 (from baseline) | Cumulative flow |
| **A_t** | \|D_t\| | Cumulative absolute change |
| **E** | Initial capital | Time-invariant (t=0ì—ì„œ ì¸¡ì •) |

---

## ğŸ”§ êµ¬í˜„ ë°©ë²•ë¡ 

### Method 1: Trajectory Tracking (ê°œë³„ ê¶¤ì  ì¶”ì )

```python
def compute_trajectories(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute company-level trajectories over time.
    """
    # Pivot to wide format for trajectory analysis
    trajectories = df.pivot(
        index='company_id', 
        columns='year', 
        values=['V', 'delta_V', 'total_delta_V']
    )
    
    # Flatten column names
    trajectories.columns = [f'{var}_{year}' for var, year in trajectories.columns]
    
    # Compute trajectory features
    trajectories['V_initial'] = trajectories['V_2021']
    trajectories['V_final'] = trajectories['V_2025']
    trajectories['total_change'] = trajectories['V_final'] - trajectories['V_initial']
    trajectories['total_movement'] = trajectories['total_delta_V_2025'].abs()
    
    # Trajectory volatility (how smooth was the path?)
    trajectories['volatility'] = trajectories[
        ['delta_V_2023', 'delta_V_2024', 'delta_V_2025']
    ].std(axis=1)
    
    # Direction consistency (did they keep moving same direction?)
    signs = np.sign(trajectories[['delta_V_2023', 'delta_V_2024', 'delta_V_2025']])
    trajectories['direction_consistency'] = signs.apply(
        lambda row: (row == row.iloc[0]).sum() / 3, axis=1
    )
    
    return trajectories
```

### Method 2: Cohort Analysis (ì½”í˜¸íŠ¸ë³„ ë¶„ì„)

```python
def cohort_analysis(df: pd.DataFrame, cohort_var: str = 'V_initial', 
                    n_cohorts: int = 4) -> pd.DataFrame:
    """
    Track cohort divergence over time.
    
    Args:
        df: Panel data (long format)
        cohort_var: Variable to define cohorts (measured at t=0)
        n_cohorts: Number of cohort groups
    """
    # Get initial values for cohort assignment
    initial = df[df['year'] == 2021][['company_id', 'V', 'first_financing_size']].copy()
    initial.columns = ['company_id', 'V_initial', 'E']
    
    # Assign cohorts
    initial['V_cohort'] = pd.qcut(initial['V_initial'], n_cohorts, 
                                   labels=['Q1_Low', 'Q2', 'Q3', 'Q4_High'])
    initial['E_cohort'] = pd.qcut(initial['E'].fillna(initial['E'].median()), n_cohorts,
                                   labels=['Q1_Low', 'Q2', 'Q3', 'Q4_High'])
    
    # Merge back to panel
    df_cohort = df.merge(initial[['company_id', 'V_cohort', 'E_cohort']], on='company_id')
    
    # Compute cohort means over time
    cohort_means = df_cohort.groupby(['year', 'V_cohort']).agg({
        'V': ['mean', 'std', 'count'],
        'total_delta_V': ['mean', 'std']
    }).reset_index()
    
    return cohort_means
```

### Method 3: Transition Analysis (ì „ì´ ë¶„ì„)

```python
def compute_transition_matrix(df: pd.DataFrame, 
                               n_states: int = 4) -> np.ndarray:
    """
    Compute Markov-style transition matrix between V states.
    
    Returns:
        Transition probability matrix P[i,j] = P(state_j at t+1 | state_i at t)
    """
    # Assign states based on V quartiles
    df = df.copy()
    df['V_state'] = pd.qcut(df['V'], n_states, labels=range(n_states))
    
    # Create transition pairs
    transitions = []
    for company in df['company_id'].unique():
        company_data = df[df['company_id'] == company].sort_values('year')
        for i in range(len(company_data) - 1):
            state_from = company_data.iloc[i]['V_state']
            state_to = company_data.iloc[i+1]['V_state']
            transitions.append((state_from, state_to))
    
    # Build transition matrix
    trans_matrix = np.zeros((n_states, n_states))
    for s_from, s_to in transitions:
        if pd.notna(s_from) and pd.notna(s_to):
            trans_matrix[int(s_from), int(s_to)] += 1
    
    # Normalize rows
    row_sums = trans_matrix.sum(axis=1, keepdims=True)
    trans_matrix = np.divide(trans_matrix, row_sums, 
                             where=row_sums != 0, out=trans_matrix)
    
    return trans_matrix
```

### Method 4: Growth Decomposition (ì„±ì¥ ë¶„í•´)

```python
def decompose_growth(df: pd.DataFrame) -> pd.DataFrame:
    """
    Decompose total change into components.
    
    D_T = Î£ Î”V_t  (total change is sum of period changes)
    A_T = Î£ |Î”V_t|  (total movement is sum of absolute changes)
    
    Efficiency = |D_T| / A_T  (how much net change per unit of movement)
    """
    trajectories = compute_trajectories(df)
    
    # Sum of absolute changes (total movement)
    trajectories['total_absolute_change'] = (
        trajectories['delta_V_2023'].abs() + 
        trajectories['delta_V_2024'].abs() + 
        trajectories['delta_V_2025'].abs()
    )
    
    # Net change
    trajectories['net_change'] = trajectories['total_delta_V_2025']
    
    # Efficiency ratio
    trajectories['pivot_efficiency'] = (
        trajectories['net_change'].abs() / 
        trajectories['total_absolute_change'].clip(lower=0.01)
    )
    
    return trajectories
```

---

## ğŸ¨ ì‹œê³„ì—´ í”Œë¡¯ ì•„ì´ë””ì–´ (7+ New Plots)

### Plot T1: Trajectory Spaghetti Plot (ê¶¤ì  ìŠ¤íŒŒê²Œí‹°)
```
ëª©ì : ê°œë³„ íšŒì‚¬ë“¤ì˜ V ë³€í™” ê¶¤ì ì„ ì‹œê°í™”
Xì¶•: Time (2021 â†’ 2023 â†’ 2024 â†’ 2025)
Yì¶•: Vagueness Score (V)
ìƒ‰ìƒ: ì´ˆê¸° Eë¡œ êµ¬ë¶„ (High E = Red, Low E = Blue)

í•µì‹¬ ì¸ì‚¬ì´íŠ¸: High-E íšŒì‚¬ë“¤ì€ ë” "í‰í‰í•œ" ê¶¤ì ì„ ë³´ì´ëŠ”ê°€?
ê¸°ëŒ€: High-E â†’ ë‚®ì€ ê¶¤ì  ë³€ë™ì„±
```

### Plot T2: Cohort Divergence (ì½”í˜¸íŠ¸ ë°œì‚°)
```
ëª©ì : ì´ˆê¸° V ì½”í˜¸íŠ¸ë³„ ì‹œê°„ì— ë”°ë¥¸ ë°œì‚° íŒ¨í„´
Xì¶•: Time
Yì¶•: Mean |Î”V| (í‰ê·  ëˆ„ì  ë³€í™”)
ì„ : ê° V ì½”í˜¸íŠ¸ (Q1-Q4)

í•µì‹¬ ì¸ì‚¬ì´íŠ¸: ì´ˆê¸°ì— ëª¨í˜¸í–ˆë˜ íšŒì‚¬ë“¤ì´ ë” ë§ì´ ì›€ì§ì´ëŠ”ê°€?
ê¸°ëŒ€: High-V ì½”í˜¸íŠ¸ â†’ ë” ê°€íŒŒë¥¸ ìƒìŠ¹ ê³¡ì„ 
```

### Plot T3: Transition Heatmap (ì „ì´ íˆíŠ¸ë§µ)
```
ëª©ì : V ìƒíƒœ ê°„ ì „ì´ í™•ë¥  ì‹œê°í™”
Xì¶•: State at t+1
Yì¶•: State at t
ìƒ‰ìƒ: ì „ì´ í™•ë¥  (0-1)

í•µì‹¬ ì¸ì‚¬ì´íŠ¸: V ìƒíƒœëŠ” ì–¼ë§ˆë‚˜ "sticky"í•œê°€?
ê¸°ëŒ€: ëŒ€ê°ì„ ì´ ë°ìŒ (ìƒíƒœ ì§€ì†), but High-VëŠ” ë” ë¶„ì‚°ë¨
```

### Plot T4: Velocity Field (ì†ë„ì¥)
```
ëª©ì : ì´ˆê¸° ìƒíƒœ(Vâ‚€, E)ì— ë”°ë¥¸ ë³€í™” ë°©í–¥ê³¼ í¬ê¸°
Xì¶•: Initial V (Vâ‚€)
Yì¶•: Initial E (log scale)
í™”ì‚´í‘œ: í‰ê·  Î”V ë°©í–¥ê³¼ í¬ê¸°

í•µì‹¬ ì¸ì‚¬ì´íŠ¸: ì–´ëŠ ì˜ì—­ì˜ íšŒì‚¬ë“¤ì´ ê°€ì¥ ë§ì´ ì›€ì§ì´ëŠ”ê°€?
ê¸°ëŒ€: Low-E, High-V ì˜ì—­ì—ì„œ ê°€ì¥ í° í™”ì‚´í‘œ
```

### Plot T5: Phase Portrait (ìœ„ìƒ ì´ˆìƒ)
```
ëª©ì : V_t vs V_{t-1} ê´€ê³„ë¡œ ë™ì—­í•™ ì‹œê°í™”
Xì¶•: V at time t-1
Yì¶•: V at time t
ëŒ€ê°ì„ : ë³€í™” ì—†ìŒ (45Â°ì„ )

í•µì‹¬ ì¸ì‚¬ì´íŠ¸: ì–´íŠ¸ë™í„°(ì•ˆì •ì )ê°€ ìˆëŠ”ê°€?
ê¸°ëŒ€: ì¤‘ê°„ê°’ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” íŒ¨í„´ (regression to mean)
```

### Plot T6: Cumulative Movement by Cohort (ëˆ„ì  ì´ë™ëŸ‰)
```
ëª©ì : ì‹œê°„ì— ë”°ë¥¸ ëˆ„ì  A_t = Î£|Î”V| ë¹„êµ
Xì¶•: Time
Yì¶•: Cumulative |Î”V|
ì„ : E ì½”í˜¸íŠ¸ë³„ (High-E vs Low-E)

í•µì‹¬ ì¸ì‚¬ì´íŠ¸: High-Eê°€ ì •ë§ ëœ ì›€ì§ì´ëŠ”ê°€?
ê¸°ëŒ€: Low-E ê³¡ì„ ì´ High-E ìœ„ì— ìœ„ì¹˜ (Golden Cage í™•ì¸)
```

### Plot T7: Sankey Diagram (ì‚°í‚¤ ë‹¤ì´ì–´ê·¸ë¨)
```
ëª©ì : V ìƒíƒœ ê°„ íšŒì‚¬ íë¦„ ì‹œê°í™”
ì™¼ìª½: 2021 V ë¶„ìœ„
ì¤‘ê°„: 2023, 2024 V ë¶„ìœ„
ì˜¤ë¥¸ìª½: 2025 V ë¶„ìœ„
íë¦„ ë„ˆë¹„: íšŒì‚¬ ìˆ˜

í•µì‹¬ ì¸ì‚¬ì´íŠ¸: íšŒì‚¬ë“¤ì´ ì–´ë””ì„œ ì–´ë””ë¡œ ì´ë™í•˜ëŠ”ê°€?
ê¸°ëŒ€: High-E íšŒì‚¬ëŠ” ë” "ì§ì„ " ê²½ë¡œ
```

### Plot T8: Event Study (ì´ë²¤íŠ¸ ìŠ¤í„°ë””)
```
ëª©ì : "í° í”¼ë²—" ì „í›„ ê¶¤ì  ë¶„ì„
Xì¶•: Time relative to pivot event (t-2, t-1, t, t+1, t+2)
Yì¶•: Mean outcome (survival, funding)
ì„ : í”¼ë²— í¬ê¸°ë³„ ì½”í˜¸íŠ¸

í•µì‹¬ ì¸ì‚¬ì´íŠ¸: í”¼ë²— í›„ ì„±ê³¼ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ê°€?
ê¸°ëŒ€: ì ì ˆí•œ í”¼ë²— â†’ ì„±ê³¼ ê°œì„ 
```

---

## ğŸ“ êµ¬í˜„ ì½”ë“œ (ì‹œê³„ì—´ í”Œë¡¯)

```python
#!/usr/bin/env python3
"""
Time Series Visualization Module for Thesis
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

def plot_T1_trajectories(df: pd.DataFrame, sample_n: int = 500,
                         save_path: Path = None) -> plt.Figure:
    """
    T1: Trajectory Spaghetti Plot
    Shows individual company V trajectories over time.
    """
    fig, ax = plt.subplots(figsize=(12, 6))
    
    # Sample companies for clarity
    companies = df['company_id'].unique()
    if len(companies) > sample_n:
        companies = np.random.choice(companies, sample_n, replace=False)
    
    # Get E for color coding
    E_data = df[df['year'] == 2021].set_index('company_id')['first_financing_size']
    E_median = E_data.median()
    
    for company in companies:
        company_data = df[df['company_id'] == company].sort_values('year')
        E = E_data.get(company, E_median)
        color = '#e74c3c' if E > E_median else '#3498db'
        alpha = 0.1
        
        ax.plot(company_data['year'], company_data['V'], 
                color=color, alpha=alpha, linewidth=0.5)
    
    # Add cohort means
    cohort_means = df.groupby('year')['V'].mean()
    ax.plot(cohort_means.index, cohort_means.values, 
            'k-', linewidth=3, label='Overall Mean')
    
    ax.set_xlabel('Year')
    ax.set_ylabel('Vagueness Score (V)')
    ax.set_title('T1: Company Trajectories in Vagueness Space\n'
                 'Red = High E, Blue = Low E')
    ax.legend()
    
    if save_path:
        fig.savefig(save_path, dpi=300, bbox_inches='tight')
    
    return fig


def plot_T2_cohort_divergence(df: pd.DataFrame, 
                              save_path: Path = None) -> plt.Figure:
    """
    T2: Cohort Divergence Plot
    Shows how V cohorts diverge in adaptive capacity over time.
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Get initial V for cohort assignment
    initial = df[df['year'] == 2021][['company_id', 'V', 'first_financing_size']]
    initial.columns = ['company_id', 'V_initial', 'E']
    
    # V cohorts
    initial['V_cohort'] = pd.qcut(initial['V_initial'], 4, 
                                   labels=['Q1 (Low V)', 'Q2', 'Q3', 'Q4 (High V)'])
    
    # E cohorts
    initial['E_cohort'] = pd.qcut(initial['E'].fillna(initial['E'].median()), 4,
                                   labels=['Q1 (Low E)', 'Q2', 'Q3', 'Q4 (High E)'])
    
    df_merged = df.merge(initial[['company_id', 'V_cohort', 'E_cohort']], on='company_id')
    
    # Left: By V cohort
    ax1 = axes[0]
    cohort_stats = df_merged.groupby(['year', 'V_cohort'])['total_delta_V'].agg(['mean', 'std']).reset_index()
    
    colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']
    for i, cohort in enumerate(['Q1 (Low V)', 'Q2', 'Q3', 'Q4 (High V)']):
        data = cohort_stats[cohort_stats['V_cohort'] == cohort]
        ax1.plot(data['year'], data['mean'].abs(), 'o-', 
                color=colors[i], linewidth=2, markersize=8, label=cohort)
        ax1.fill_between(data['year'], 
                        (data['mean'] - data['std']).abs(),
                        (data['mean'] + data['std']).abs(),
                        color=colors[i], alpha=0.2)
    
    ax1.set_xlabel('Year')
    ax1.set_ylabel('Mean |Î”V| (Cumulative Movement)')
    ax1.set_title('Cohort Divergence by Initial Vagueness')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Right: By E cohort
    ax2 = axes[1]
    cohort_stats_E = df_merged.groupby(['year', 'E_cohort'])['total_delta_V'].agg(['mean', 'std']).reset_index()
    
    for i, cohort in enumerate(['Q1 (Low E)', 'Q2', 'Q3', 'Q4 (High E)']):
        data = cohort_stats_E[cohort_stats_E['E_cohort'] == cohort]
        ax2.plot(data['year'], data['mean'].abs(), 'o-',
                color=colors[i], linewidth=2, markersize=8, label=cohort)
    
    ax2.set_xlabel('Year')
    ax2.set_ylabel('Mean |Î”V| (Cumulative Movement)')
    ax2.set_title('Cohort Divergence by Initial Capital\n'
                  'Expected: Low E (Blue) > High E (Red)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        fig.savefig(save_path, dpi=300, bbox_inches='tight')
    
    return fig


def plot_T3_transition_heatmap(df: pd.DataFrame, n_states: int = 4,
                               save_path: Path = None) -> plt.Figure:
    """
    T3: Transition Matrix Heatmap
    Shows probabilities of moving between V states.
    """
    fig, ax = plt.subplots(figsize=(8, 7))
    
    # Compute transition matrix
    df = df.copy()
    df['V_state'] = pd.qcut(df['V'], n_states, 
                            labels=[f'Q{i+1}' for i in range(n_states)])
    
    # Build transitions
    transitions = []
    for company in df['company_id'].unique():
        company_data = df[df['company_id'] == company].sort_values('year')
        states = company_data['V_state'].values
        for i in range(len(states) - 1):
            if pd.notna(states[i]) and pd.notna(states[i+1]):
                transitions.append((states[i], states[i+1]))
    
    # Create matrix
    state_labels = [f'Q{i+1}' for i in range(n_states)]
    trans_df = pd.DataFrame(transitions, columns=['from', 'to'])
    trans_matrix = pd.crosstab(trans_df['from'], trans_df['to'], normalize='index')
    trans_matrix = trans_matrix.reindex(index=state_labels, columns=state_labels, fill_value=0)
    
    # Heatmap
    sns.heatmap(trans_matrix, annot=True, fmt='.2f', cmap='Blues',
                ax=ax, vmin=0, vmax=1,
                xticklabels=['Q1\n(Low V)', 'Q2', 'Q3', 'Q4\n(High V)'],
                yticklabels=['Q1\n(Low V)', 'Q2', 'Q3', 'Q4\n(High V)'])
    
    ax.set_xlabel('State at t+1')
    ax.set_ylabel('State at t')
    ax.set_title('T3: Transition Probabilities Between Vagueness States\n'
                 'Diagonal = persistence, Off-diagonal = change')
    
    if save_path:
        fig.savefig(save_path, dpi=300, bbox_inches='tight')
    
    return fig


def plot_T6_cumulative_movement(df: pd.DataFrame, 
                                 save_path: Path = None) -> plt.Figure:
    """
    T6: Cumulative Movement by E Cohort
    THE KEY PLOT for Golden Cage hypothesis in time series form.
    """
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Get E cohorts
    initial = df[df['year'] == 2021][['company_id', 'first_financing_size']].copy()
    initial.columns = ['company_id', 'E']
    initial['E_cohort'] = pd.qcut(initial['E'].fillna(initial['E'].median()), 2,
                                   labels=['Low E', 'High E'])
    
    df_merged = df.merge(initial[['company_id', 'E_cohort']], on='company_id')
    
    # Compute cumulative |Î”V| over time
    cumulative = df_merged.groupby(['year', 'E_cohort']).apply(
        lambda x: x['total_delta_V'].abs().mean()
    ).reset_index()
    cumulative.columns = ['year', 'E_cohort', 'mean_A']
    
    # Plot
    colors = {'Low E': '#3498db', 'High E': '#e74c3c'}
    for cohort in ['Low E', 'High E']:
        data = cumulative[cumulative['E_cohort'] == cohort]
        ax.plot(data['year'], data['mean_A'], 'o-',
                color=colors[cohort], linewidth=3, markersize=10, 
                label=cohort)
    
    # Add gap annotation
    final_year = cumulative['year'].max()
    low_E_final = cumulative[(cumulative['E_cohort'] == 'Low E') & 
                             (cumulative['year'] == final_year)]['mean_A'].values[0]
    high_E_final = cumulative[(cumulative['E_cohort'] == 'High E') & 
                              (cumulative['year'] == final_year)]['mean_A'].values[0]
    gap = low_E_final - high_E_final
    
    ax.annotate(f'Gap = {gap:.2f}',
                xy=(final_year, (low_E_final + high_E_final)/2),
                xytext=(final_year - 0.5, (low_E_final + high_E_final)/2 + 2),
                fontsize=12, fontweight='bold',
                arrowprops=dict(arrowstyle='->', color='black'))
    
    ax.set_xlabel('Year')
    ax.set_ylabel('Mean Cumulative |Î”V| (Adaptive Capacity)')
    ax.set_title('T6: Golden Cage in Time Series Form\n'
                 'Expected: Low E (Blue) > High E (Red)')
    ax.legend(loc='upper left')
    ax.grid(True, alpha=0.3)
    
    # Add interpretation
    result = "âœ“ GOLDEN CAGE CONFIRMED" if gap > 0 else "âœ— Not confirmed"
    ax.text(0.98, 0.02, result, transform=ax.transAxes,
            fontsize=14, fontweight='bold', ha='right', va='bottom',
            color='#27ae60' if gap > 0 else '#e74c3c',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    if save_path:
        fig.savefig(save_path, dpi=300, bbox_inches='tight')
    
    return fig


def generate_all_timeseries_plots(df: pd.DataFrame, output_dir: Path):
    """Generate all time series plots."""
    print("="*70)
    print("ğŸ… Time Series Visualization Module")
    print("="*70)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\nğŸ“ˆ Generating time series plots...")
    
    plot_T1_trajectories(df, save_path=output_dir / 'T1_trajectories.png')
    print("   âœ… T1: Trajectory Spaghetti")
    
    plot_T2_cohort_divergence(df, save_path=output_dir / 'T2_cohort_divergence.png')
    print("   âœ… T2: Cohort Divergence")
    
    plot_T3_transition_heatmap(df, save_path=output_dir / 'T3_transition_heatmap.png')
    print("   âœ… T3: Transition Heatmap")
    
    plot_T6_cumulative_movement(df, save_path=output_dir / 'T6_cumulative_movement.png')
    print("   âœ… T6: Cumulative Movement (Golden Cage)")
    
    print(f"\nâœ… All time series plots saved to: {output_dir}")


if __name__ == "__main__":
    # Load data
    ROOT = Path("/Users/hyunjimoon/tolzul/Front/On/love(cs)/strategic_ambiguity/empirics")
    df = pd.read_parquet(ROOT / "data/processed/vagueness_timeseries.parquet")
    
    generate_all_timeseries_plots(
        df, 
        ROOT / "src/scripts/paper_generation/output/_thesis_package/figures/timeseries"
    )
```

---

## ğŸ“Š í”Œë¡¯ ìš”ì•½ ë§¤íŠ¸ë¦­ìŠ¤

| Plot ID | ì´ë¦„ | Xì¶• | Yì¶• | í•µì‹¬ ì§ˆë¬¸ | ê¸°ëŒ€ ê²°ê³¼ |
|---------|------|-----|-----|----------|----------|
| **T1** | Trajectory Spaghetti | Time | V | ê°œë³„ ê¶¤ì  íŒ¨í„´? | High-E = í‰í‰í•œ ê¶¤ì  |
| **T2** | Cohort Divergence | Time | Mean \|Î”V\| | ì½”í˜¸íŠ¸ ë°œì‚°? | Low-Eê°€ ë” ë§ì´ ë°œì‚° |
| **T3** | Transition Heatmap | State(t+1) | State(t) | ìƒíƒœ ì§€ì†ì„±? | High-VëŠ” ë” ìœ ë™ì  |
| **T4** | Velocity Field | Vâ‚€ | E | ì–´ë””ì„œ ê°€ì¥ ì›€ì§ì„? | Low-E, High-V ì˜ì—­ |
| **T5** | Phase Portrait | V(t-1) | V(t) | ì–´íŠ¸ë™í„° ì¡´ì¬? | ì¤‘ê°„ê°’ ìˆ˜ë ´ |
| **T6** | Cumulative Movement | Time | Cumulative A | Golden Cage ì‹œê³„ì—´? | Low-E ê³¡ì„ ì´ ìœ„ |
| **T7** | Sankey Diagram | Time | State flow | íë¦„ íŒ¨í„´? | High-E = ì§ì„  |
| **T8** | Event Study | Relative time | Outcome | í”¼ë²— íš¨ê³¼? | ì ì ˆí•œ í”¼ë²— â†’ ê°œì„  |

---

## ğŸ¯ í•µì‹¬ ì‹œê³„ì—´ ì¸ì‚¬ì´íŠ¸

### Golden Cageì˜ ì‹œê°„ì  ì¦ê±°
```
t=0: High-Eì™€ Low-E íšŒì‚¬ë“¤ì´ ìœ ì‚¬í•œ V ë¶„í¬ë¡œ ì‹œì‘
t=1: Low-E íšŒì‚¬ë“¤ì´ ë” ë§ì´ ì´ë™í•˜ê¸° ì‹œì‘
t=2: Gap í™•ëŒ€
t=3: Low-E íšŒì‚¬ë“¤ì´ ëˆ„ì ì ìœ¼ë¡œ í›¨ì”¬ ë” ë§ì´ ì´ë™í•¨

ê²°ë¡ : MoneyëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì ì  ë” ê°•í•œ "cage"ê°€ ë¨
```

### U-Shapeì˜ ë™ì  ê²€ì¦
```
ì´ˆê¸° High-V íšŒì‚¬: ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ V ê°ì†Œ (êµ¬ì²´í™”)
ì´ˆê¸° Low-V íšŒì‚¬: ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ V ì¦ê°€ (í™•ì¥) OR ìœ ì§€
ì´ˆê¸° Mid-V íšŒì‚¬: ë°©í–¥ì„± ì—†ëŠ” ë³€ë™

ê²°ë¡ : ê·¹ë‹¨ì  VëŠ” "ëª©ì ì§€"ë¡œ ê¸°ëŠ¥, ì¤‘ê°„ VëŠ” "í†µê³¼ì "
```

---

**END OF TIME SERIES GUIDE**
