#!/usr/bin/env python3
"""
VALIDATION TEST: Vagueness Scorer Data Quality
===============================================

PURPOSE: Validate the vagueness scorer behavior and data pipeline.
This test file is designed for independent verification by another LLM or reviewer.

CRITICAL FINDING: V=87.5 appears in 43% of all data!

TEST OBJECTIVES:
1. Verify V=87.5 formula derivation
2. Test scorer behavior on various text types
3. Analyze raw data distribution
4. Provide recommendations for data quality

Date: 2025-12-20
Context: Î¨¥Ï°∞Í±¥ Ï†ïÌôïÏÑ±Ïù¥ ÏµúÏö∞ÏÑ† - data validation for thesis
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from collections import Counter

# Add src to path
REPO_ROOT = Path("/Users/hyunjimoon/tolzul/Front/On/love(cs)/strategic_ambiguity/empirics")
sys.path.insert(0, str(REPO_ROOT / "src"))

from vagueness_v2 import HybridVaguenessScorerV2, StrategicVaguenessScorerV2


def test_v875_formula():
    """
    TEST 1: Verify that V=87.5 comes from the expected formula.

    Expected: When text has NO abstract terms (S_cat=0) and NO concrete features,
    - S_concdef = 100 (maximum concreteness deficit)
    - V_raw = 0.5 * max(0, 100) + 0.5 * mean(0, 100) = 0.5*100 + 0.5*50 = 75
    - concrete_score = 100 (no concrete features found)
    - hybrid_score = 0.5 * 75 + 0.5 * 100 = 87.5
    """
    print("\n" + "="*60)
    print("TEST 1: V=87.5 Formula Verification")
    print("="*60)

    scorer = HybridVaguenessScorerV2()

    # Test cases that should produce V=87.5
    test_texts = [
        "",                           # Empty string
        "short",                      # Very short text
        "company description",        # Generic words only
        "we are a company",           # No abstract terms, no concrete features
        "building the future",        # Vague but no abstract terms from lexicon
    ]

    print("\nTexts expected to produce V‚âà87.5:")
    for text in test_texts:
        score = scorer.score(text)
        status = "‚úì" if abs(score - 87.5) < 0.5 else "‚úó"
        print(f"  {status} V={score:.2f} | '{text[:50]}'")

    # Decompose the scoring for the empty string
    print("\n--- Decomposition for empty string ---")
    v2 = StrategicVaguenessScorerV2()
    v2.fit([""])
    result = v2.transform([""])
    print(f"  S_cat = {result['S_cat'].iloc[0]:.2f}")
    print(f"  S_concdef = {result['S_concdef'].iloc[0]:.2f}")
    print(f"  V_raw = {result['V_raw'].iloc[0]:.2f}")

    # Verify concrete_score calculation
    concrete_count = scorer._count_concrete_features("")
    print(f"  concrete_count = {concrete_count}")
    concrete_score = 100 - min(concrete_count / max(1, scorer.max_concrete) * 100, 100)
    print(f"  concrete_score = {concrete_score:.2f}")

    expected_hybrid = 0.5 * result['V_raw'].iloc[0] + 0.5 * concrete_score
    print(f"  Expected hybrid = 0.5 * {result['V_raw'].iloc[0]:.2f} + 0.5 * {concrete_score:.2f} = {expected_hybrid:.2f}")

    return True


def test_text_differentiation():
    """
    TEST 2: Verify scorer can differentiate between clear vs vague texts.

    A good scorer should:
    - Score specific technical text LOW (concrete)
    - Score vague marketing text HIGH (vague)
    - Score empty/generic text HIGH (vague by default)
    """
    print("\n" + "="*60)
    print("TEST 2: Text Differentiation Capability")
    print("="*60)

    scorer = HybridVaguenessScorerV2()

    # Concrete (specific) texts - should score LOW
    concrete_texts = [
        "We developed a 7nm chip achieving 10 TFLOPS at 45W power consumption. Production started Q2 2024.",
        "Our database processes 1.2 million queries per second with 99.99% uptime. Deployed at 50 Fortune 500 companies.",
        "Version 3.2 released January 2024 with 40% latency reduction. Benchmark: 2ms response time on 10GB datasets.",
    ]

    # Vague (abstract) texts - should score HIGH
    vague_texts = [
        "We leverage AI to create seamless, next-generation solutions that transform the digital landscape.",
        "Our platform enables holistic, end-to-end optimization through innovative strategic frameworks.",
        "Revolutionary ecosystem driving transformative value through cutting-edge, intelligent automation.",
    ]

    # Fit on all texts
    all_texts = concrete_texts + vague_texts
    scorer.fit(all_texts)

    print("\n--- Concrete texts (should be LOW) ---")
    for text in concrete_texts:
        score = scorer.score(text)
        print(f"  V={score:5.1f} | '{text[:60]}...'")

    print("\n--- Vague texts (should be HIGH) ---")
    for text in vague_texts:
        score = scorer.score(text)
        print(f"  V={score:5.1f} | '{text[:60]}...'")

    # Calculate mean scores
    concrete_scores = [scorer.score(t) for t in concrete_texts]
    vague_scores = [scorer.score(t) for t in vague_texts]

    print(f"\nMean concrete score: {np.mean(concrete_scores):.1f}")
    print(f"Mean vague score: {np.mean(vague_scores):.1f}")
    print(f"Separation: {np.mean(vague_scores) - np.mean(concrete_scores):.1f} points")

    # Test passes if vague texts score higher than concrete texts
    passed = np.mean(vague_scores) > np.mean(concrete_scores)
    print(f"\nResult: {'PASS' if passed else 'FAIL'} - Vague texts score higher than concrete texts")

    return passed


def test_raw_data_distribution():
    """
    TEST 3: Analyze the actual vagueness_timeseries.parquet data.

    CRITICAL: 43% of V values are exactly 87.5. Is this valid or a bug?
    """
    print("\n" + "="*60)
    print("TEST 3: Raw Data Distribution Analysis")
    print("="*60)

    data_path = REPO_ROOT / "data" / "processed" / "vagueness_timeseries.parquet"

    if not data_path.exists():
        print(f"  WARNING: Data file not found: {data_path}")
        return False

    df = pd.read_parquet(data_path)
    print(f"\nTotal rows: {len(df):,}")
    print(f"Unique companies: {df['company_id'].nunique():,}")
    print(f"Years: {sorted(df['year'].unique())}")

    # V distribution
    print("\n--- V Score Distribution ---")
    v_counts = df['V'].value_counts().head(10)
    print("Top 10 most frequent V values:")
    for v, count in v_counts.items():
        pct = count / len(df) * 100
        print(f"  V={v:6.2f}: {count:>8,} ({pct:5.2f}%)")

    # V=87.5 analysis
    v875_count = (df['V'] == 87.5).sum()
    v875_pct = v875_count / len(df) * 100
    print(f"\nüö® V=87.5 appears in {v875_count:,} rows ({v875_pct:.1f}%)")

    # Analyze V=87.5 by year
    print("\n--- V=87.5 by Year ---")
    for year in sorted(df['year'].unique()):
        year_df = df[df['year'] == year]
        v875_year = (year_df['V'] == 87.5).sum()
        pct = v875_year / len(year_df) * 100
        print(f"  {year}: {v875_year:>8,} / {len(year_df):,} ({pct:.1f}%)")

    # Other common values near 87.5
    print("\n--- Values in 85-90 range ---")
    range_mask = (df['V'] >= 85) & (df['V'] <= 90)
    range_counts = df.loc[range_mask, 'V'].value_counts()
    for v, count in range_counts.head(10).items():
        print(f"  V={v:.2f}: {count:,}")

    # Statistical summary
    print("\n--- V Statistics ---")
    print(f"  Mean: {df['V'].mean():.2f}")
    print(f"  Median: {df['V'].median():.2f}")
    print(f"  Std: {df['V'].std():.2f}")
    print(f"  Min: {df['V'].min():.2f}")
    print(f"  Max: {df['V'].max():.2f}")

    # Percentiles
    print("\n--- V Percentiles ---")
    for p in [10, 25, 50, 75, 90, 95, 99]:
        print(f"  {p}th: {df['V'].quantile(p/100):.2f}")

    return v875_pct < 50  # Pass if less than 50% are 87.5


def test_source_text_for_v875():
    """
    TEST 4: Trace V=87.5 back to source texts.

    What kind of company descriptions lead to V=87.5?
    """
    print("\n" + "="*60)
    print("TEST 4: Source Text Analysis for V=87.5 Companies")
    print("="*60)

    # Try to load raw text data
    vag_path = REPO_ROOT / "data" / "processed" / "vagueness_timeseries.parquet"
    desc_path = REPO_ROOT / "data" / "raw" / "company_descriptions.parquet"

    if not vag_path.exists():
        print("  Data not available")
        return False

    vag = pd.read_parquet(vag_path)
    v875_companies = vag[vag['V'] == 87.5]['company_id'].unique()[:20]

    print(f"\nV=87.5 companies sampled: {len(v875_companies)}")

    # Try to load company info
    feat_path = REPO_ROOT / "data" / "processed" / "features_all.parquet"
    if feat_path.exists():
        feat = pd.read_parquet(feat_path)
        print("\n--- Sample V=87.5 Companies ---")

        # Find these companies in features
        for cid in v875_companies[:5]:
            company_info = feat[feat['CompanyID'].astype(str) == str(cid)]
            if len(company_info) > 0:
                name = company_info.iloc[0].get('Name', 'Unknown')
                desc = company_info.iloc[0].get('ShortDescription', '')
                if isinstance(desc, str):
                    desc_preview = desc[:100] + "..." if len(desc) > 100 else desc
                else:
                    desc_preview = "(no description)"
                print(f"\n  ID: {cid}")
                print(f"  Name: {name}")
                print(f"  Desc: {desc_preview}")

    return True


def test_expected_v_range():
    """
    TEST 5: Test what V values we should expect for typical startup descriptions.
    """
    print("\n" + "="*60)
    print("TEST 5: Expected V Range for Typical Startups")
    print("="*60)

    # Real-ish startup descriptions
    typical_descriptions = [
        # Fintech
        "Mobile payment platform for emerging markets. $2M ARR, 100K users.",
        "AI-powered credit scoring for underbanked populations. Partnership with 3 banks.",

        # SaaS
        "Cloud-based HR management software. SOC2 certified. 500 enterprise customers.",
        "Project management tool with Slack integration. 10,000 DAU.",

        # Healthcare
        "FDA-cleared diagnostic device for early cancer detection. 95% accuracy in clinical trials.",
        "Telemedicine platform serving 50 clinics. $5M Series A.",

        # Vague/early stage
        "We are building the future of work.",
        "Innovative solution for enterprise needs.",
        "AI-powered platform.",
        "",  # Empty description (common for very early companies)
    ]

    scorer = HybridVaguenessScorerV2()
    scorer.fit(typical_descriptions)

    print("\n--- Typical Startup Descriptions ---")
    for desc in typical_descriptions:
        score = scorer.score(desc)
        preview = desc[:50] + "..." if len(desc) > 50 else desc if desc else "(empty)"
        print(f"  V={score:5.1f} | {preview}")

    # Count how many produce V=87.5
    v875_count = sum(1 for d in typical_descriptions if abs(scorer.score(d) - 87.5) < 0.5)
    print(f"\n  {v875_count} / {len(typical_descriptions)} produce V‚âà87.5")

    return True


def generate_validation_report():
    """
    Generate a comprehensive validation report.
    """
    print("\n" + "="*60)
    print("VALIDATION REPORT: Vagueness Scorer")
    print("="*60)

    results = {}

    # Run all tests
    results['formula'] = test_v875_formula()
    results['differentiation'] = test_text_differentiation()
    results['distribution'] = test_raw_data_distribution()
    results['source_text'] = test_source_text_for_v875()
    results['expected_range'] = test_expected_v_range()

    # Summary
    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)

    for test, passed in results.items():
        status = "‚úì PASS" if passed else "‚úó FAIL"
        print(f"  {test}: {status}")

    # Recommendations
    print("\n" + "="*60)
    print("FINDINGS & RECOMMENDATIONS")
    print("="*60)

    print("""
1. V=87.5 IS EXPECTED BEHAVIOR for texts without:
   - Abstract buzzwords (platform, solution, etc.)
   - Concrete features (numbers, dates, units)

2. THE ISSUE IS NOT A BUG in the scorer.
   The issue is that 43% of company descriptions lack both:
   - Marketing jargon (so S_cat=0)
   - Specific data (so S_concdef=100, concrete_score=100)

3. THIS MAY BE VALID for very early-stage companies that:
   - Have minimal public descriptions
   - Haven't developed marketing language yet
   - Lack quantifiable metrics to share

4. POSSIBLE ACTIONS:
   a) Accept: V=87.5 represents "no signal" - neither specific nor vague
   b) Filter: Exclude companies with V=87.5 from analysis as "uninformative"
   c) Impute: Use industry/stage average for V=87.5 companies
   d) Redefine: Treat V=87.5 as "moderate vagueness" baseline

5. FOR THESIS VALIDITY:
   - Clearly document that 43% of companies have baseline vagueness
   - Consider sensitivity analysis excluding these companies
   - The archetype classification (zoom_in/out/stayer) may be less reliable
     for companies stuck at V=87.5 across years
""")

    return all(results.values())


if __name__ == "__main__":
    print("="*60)
    print("VAGUENESS SCORER VALIDATION TEST SUITE")
    print("="*60)
    print("This test validates data quality for thesis research.")
    print("Running all tests...")

    passed = generate_validation_report()

    print("\n" + "="*60)
    print(f"OVERALL: {'ALL TESTS PASSED' if passed else 'SOME TESTS FAILED'}")
    print("="*60)

    sys.exit(0 if passed else 1)
